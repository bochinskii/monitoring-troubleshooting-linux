----
DISK
----




Не информативно

$ vmstat
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 390404 191928 705148    0    0    17    57   45   66  0  1 99  0  0

io (все что связано с дисковой подсистемой)
	bi - сколько блоков считали с диска (17)
	bo - сколько блоков записали на диск (57)

размер блока зависит от файловой системы






Информативно, можно вполне отдебажить

$ sudo iotop -k

Total DISK READ:         0.00 K/s | Total DISK WRITE:       213.96 K/s
Current DISK READ:       0.00 K/s | Current DISK WRITE:     280.60 K/s
  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO>    COMMAND
  254 be/3 root        0.00 K/s    0.00 K/s  0.00 %  0.18 % [jbd2/sda2-8]
  834 be/4 mysql       0.00 K/s   10.52 K/s  0.00 %  0.08 % mysqld
 1089 be/4 mysql       0.00 K/s    3.51 K/s  0.00 %  0.03 % mysqld
  821 be/4 mysql       0.00 K/s  196.42 K/s  0.00 %  0.03 % mysqld
  820 be/4 mysql       0.00 K/s    0.00 K/s  0.00 %  0.03 % mysqld
  812 be/4 mysql       0.00 K/s    0.00 K/s  0.00 %  0.01 % mysqld
 1088 be/4 mysql       0.00 K/s    3.51 K/s  0.00 %  0.01 % mysqld
    1 be/4 root        0.00 K/s    0.00 K/s  0.00 %  0.00 % init
    2 be/4 root        0.00 K/s    0.00 K/s  0.00 %  0.00 % [kthreadd]
    3 be/0 root        0.00 K/s    0.00 K/s  0.00 %  0.00 % [rcu_gp]
    4 be/0 root        0.00 K/s    0.00 K/s  0.00 %  0.00 % [rcu_par_gp]
    6 be/0 root        0.00 K/s    0.00 K/s  0.00 %  0.00 % [kworker/0:0H-kblockd]

Тут важно - Current DISK READ и Current DISK WRITE: 280 K/s

TID - номер потока. По нему можно вычеслить какой процесс загружает дисковую подсистему
PRIO - приоритет
  be - best efford (0 - 7) - 0 - самый лучший приоритет
  rt - real time (0 - 7) - 0 - самый лучший приоритет





Не плохой инструмент. Можно отсортировать по состоянию процессов s State (S). Если будет буква - D, это чаще всего значит, что процесс ждет ответа от диска, реже от памяти.
Если использовать дополнительно, то можно вполне потять что за процесс, а то и команда (что очень важно) загружает подсистему.

$ htop
 
 
 
 
 
Вот так удобно запускать данную утилиту

$ iostat -xk -t 10 | awk '// {print strftime("%Y-%m-%d %H:%M:%S"),$0}'

Собирать и выводить информацию каждые 10 секунд 

2022-07-09 15:58:18 Linux 4.19.0-6-amd64 (zab2)         07/09/2022      _x86_64_        (6 CPU)
2022-07-09 15:58:18
2022-07-09 15:58:18 07/09/2022 03:58:18 PM
2022-07-09 15:58:18 avg-cpu:  %user   %nice %system %iowait  %steal   %idle
2022-07-09 15:58:18            0.52    0.00    1.01    0.02    0.00   98.45
2022-07-09 15:58:18
2022-07-09 15:58:18 Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
2022-07-09 15:58:18 sda              2.41   16.92     77.81    324.89     0.39     5.86  14.04  25.72    1.41    0.64   0.01    32.28    19.20   0.46   0.90
2022-07-09 15:58:18


- пропускная способность (колонки rkB/s, wkB/s) - 77.81, 324.89
- iops (колонки r/s, w/s) — количество чтения/записи в секунду (2.41, 16,92) (важный парамметр при больших нагрузках)
- время обработки запроса: await — 1,41 ms (диск обрабатывает запрос на вот такое время), из них собственно выполнение svctm 0,46 ms
- aqu-sz – средняя длина очереди - 0.01 (важный парамметр)
- util - утилизация диска (важный парамметр)






$ sar -b 10 1

Linux 4.19.0-6-amd64 (zab2)     07/09/2022      _x86_64_        (6 CPU)

04:07:27 PM       tps      rtps      wtps   bread/s   bwrtn/s
04:07:37 PM     15.38      0.00     15.38      0.00    438.76
Average:        15.38      0.00     15.38      0.00    438.76


Собирать информацию 10 секунд и вывести 1 раз

tps, rtps, wtps - запросы в сукунду
bread, bwrtn - количество блоков в секунду.




Создать нагрузку на диск

$ while true; do dd if=/dev/urandom count=10M bs=1 | bzip2 -9 > /tmp/random.bin ; rm -f /tmp/random.bin ; done

или

Используем 1-го воркера на операцию вызова IO и 1-го воркера для записи 512 M/B данных

$ stress -i 1 -d 1 --hdd-bytes 512M